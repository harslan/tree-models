{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸŒ² Tree-Based Models in Prescriptive Analytics\n",
    "## ISOM 839: From Prediction to Optimization\n",
    "\n",
    "**Tonight's Journey:**\n",
    "```\n",
    "Better Predictions â†’ Better Decisions â†’ Better Business Outcomes\n",
    "```\n",
    "\n",
    "**Learning Objectives:**\n",
    "1. Understand why ML model choice impacts optimization results\n",
    "2. Master Random Forests and Gradient Boosting for demand prediction\n",
    "3. Integrate tree-based models with Gurobi optimization\n",
    "4. Critically evaluate prediction-optimization trade-offs\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ The Central Question\n",
    "\n",
    "**Last week:** You built a price optimization model using Linear Regression\n",
    "\n",
    "**Tonight:** What if we could predict demand MORE accurately? Would our optimal prices change?\n",
    "\n",
    "### The Prescriptive Analytics Pipeline\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Historical     â”‚      â”‚   ML Model       â”‚      â”‚  Optimization   â”‚\n",
    "â”‚  Data           â”‚â”€â”€â”€â”€â”€â–¶â”‚  (Prediction)    â”‚â”€â”€â”€â”€â”€â–¶â”‚  Model          â”‚\n",
    "â”‚                 â”‚      â”‚                  â”‚      â”‚  (Decision)     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "     Descriptive            Predictive              Prescriptive\n",
    "```\n",
    "\n",
    "**Key Insight:** The quality of your predictions directly affects the quality of your decisions!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - Run this first!\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Optimization Libraries\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "import gurobipy_pandas as gppd\n",
    "from gurobi_ml import add_predictor_constr\n",
    "\n",
    "# Visualization\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ… All libraries loaded successfully!\")\n",
    "print(f\"ğŸ”¹ Gurobi version: {gp.gurobi.version()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Why Model Choice Matters\n",
    "\n",
    "## ğŸ“Š Load the Avocado Data (Quick Review)\n",
    "\n",
    "Let's start with the same dataset you've been working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HAB avocado data\n",
    "data_url = \"https://raw.githubusercontent.com/Gurobi/modeling-examples/master/price_optimization/\"\n",
    "avocado = pd.read_csv(data_url+\"HAB_data_2015to2022.csv\")\n",
    "avocado[\"date\"] = pd.to_datetime(avocado[\"date\"])\n",
    "avocado = avocado.sort_values(by=\"date\")\n",
    "\n",
    "regions = [\n",
    "    \"Great_Lakes\", \"Midsouth\", \"Northeast\", \"Northern_New_England\",\n",
    "    \"SouthCentral\", \"Southeast\", \"West\", \"Plains\"\n",
    "]\n",
    "df = avocado[avocado.region.isin(regions)]\n",
    "\n",
    "print(f\"ğŸ“¦ Dataset loaded: {len(df):,} observations\")\n",
    "print(f\"ğŸ“… Date range: {df['date'].min().date()} to {df['date'].max().date()}\")\n",
    "print(f\"ğŸŒ Regions: {len(regions)}\")\n",
    "print(f\"\\nğŸ¯ Our goal: Predict 'units_sold' using 'price', 'region', 'year', 'peak'\")\n",
    "\n",
    "df[['date', 'region', 'price', 'units_sold', 'year', 'peak']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = df[[\"region\", \"price\", \"year\", \"peak\"]]\n",
    "y = df[\"units_sold\"]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.8, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"âœ… Training set: {len(X_train):,} samples\")\n",
    "print(f\"âœ… Testing set: {len(X_test):,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸŒ³ Section 1: Decision Trees - The Building Block\n",
    "\n",
    "### What is a Decision Tree?\n",
    "\n",
    "A decision tree is like a flowchart that asks yes/no questions about your data:\n",
    "\n",
    "```\n",
    "                    Is price < $1.50?\n",
    "                    /              \\\n",
    "                  YES              NO\n",
    "                  /                 \\\n",
    "         Is peak season?        Is region = West?\n",
    "           /      \\                /          \\\n",
    "         YES      NO             YES          NO\n",
    "          |        |              |            |\n",
    "       Predict   Predict       Predict     Predict\n",
    "       3.2M      2.1M          1.8M        1.5M\n",
    "```\n",
    "\n",
    "### Advantages:\n",
    "- âœ… Easy to interpret (\"If price is low AND it's peak season, then...\")\n",
    "- âœ… Handles non-linear relationships automatically\n",
    "- âœ… No need to normalize features\n",
    "\n",
    "### Limitations:\n",
    "- âŒ **High variance** - small changes in data can dramatically change the tree\n",
    "- âŒ **Overfitting** - can memorize training data rather than learn patterns\n",
    "- âŒ **Unstable** - not reliable for making business decisions alone\n",
    "\n",
    "### The Solution: Ensemble Methods ğŸŒ²ğŸŒ²ğŸŒ²\n",
    "\n",
    "**Core idea:** Instead of asking ONE expert, ask MANY experts and combine their answers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build preprocessing pipeline (same as before)\n",
    "feat_transform = make_column_transformer(\n",
    "    (OneHotEncoder(drop=\"first\"), [\"region\"]),\n",
    "    (StandardScaler(), [\"price\", \"year\"]),\n",
    "    (\"passthrough\", [\"peak\"]),\n",
    "    verbose_feature_names_out=False,\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Train a single decision tree\n",
    "tree_pipeline = make_pipeline(feat_transform, DecisionTreeRegressor(max_depth=10, random_state=42))\n",
    "tree_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "tree_pred = tree_pipeline.predict(X_test)\n",
    "tree_r2 = r2_score(y_test, tree_pred)\n",
    "tree_mae = mean_absolute_error(y_test, tree_pred)\n",
    "\n",
    "print(\"ğŸŒ³ Single Decision Tree Performance:\")\n",
    "print(f\"   RÂ² Score: {tree_r2:.4f}\")\n",
    "print(f\"   Mean Absolute Error: {tree_mae:.4f} million avocados\")\n",
    "print(f\"\\nğŸ’¡ Interpretation: On average, our predictions are off by {tree_mae*1000000:,.0f} avocados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’­ Discussion Question:\n",
    "\n",
    "**Q:** Is an RÂ² of ~0.85 \"good enough\" for business decisions involving millions of dollars?\n",
    "\n",
    "**Think about:**\n",
    "- What's the cost of overestimating demand? (wasted avocados)\n",
    "- What's the cost of underestimating demand? (lost sales)\n",
    "- Could we do better?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸŒ²ğŸŒ²ğŸŒ² Section 2: Random Forest - Wisdom of the Crowd\n",
    "\n",
    "### The Core Idea\n",
    "\n",
    "**Question:** Would you rather ask:\n",
    "- ONE financial analyst about a stock price?\n",
    "- 100 financial analysts, then average their predictions?\n",
    "\n",
    "**Random Forest** = Build many decision trees and average their predictions\n",
    "\n",
    "### How It Works:\n",
    "\n",
    "1. **Bootstrap Sampling** (Bagging):\n",
    "   - Create 100 different training datasets by randomly sampling WITH replacement\n",
    "   - Each tree sees a slightly different view of the data\n",
    "\n",
    "2. **Random Feature Selection**:\n",
    "   - At each split, only consider a random subset of features\n",
    "   - This decorrelates the trees (makes them more independent)\n",
    "\n",
    "3. **Aggregation**:\n",
    "   - For regression: **Average** all tree predictions\n",
    "   - For classification: **Majority vote**\n",
    "\n",
    "### Why This Works:\n",
    "\n",
    "**Individual tree:** High variance, prone to overfitting  \n",
    "**Average of many trees:** Variance cancels out! More stable predictions.\n",
    "\n",
    "**Mathematical intuition:**\n",
    "- If each tree has error $\\epsilon$\n",
    "- Average of $n$ independent trees has error $\\epsilon/\\sqrt{n}$\n",
    "- More trees = More stable predictions\n",
    "\n",
    "### Key Hyperparameters:\n",
    "\n",
    "```python\n",
    "RandomForestRegressor(\n",
    "    n_estimators=100,      # Number of trees (more is usually better)\n",
    "    max_depth=None,        # How deep each tree can grow\n",
    "    min_samples_split=2,   # Minimum samples to split a node\n",
    "    max_features='sqrt',   # Features to consider at each split\n",
    "    random_state=42        # For reproducibility\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest\n",
    "rf_pipeline = make_pipeline(\n",
    "    feat_transform, \n",
    "    RandomForestRegressor(n_estimators=100, max_depth=15, random_state=42, n_jobs=-1)\n",
    ")\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "rf_pred = rf_pipeline.predict(X_test)\n",
    "rf_r2 = r2_score(y_test, rf_pred)\n",
    "rf_mae = mean_absolute_error(y_test, rf_pred)\n",
    "\n",
    "print(\"ğŸŒ²ğŸŒ²ğŸŒ² Random Forest Performance:\")\n",
    "print(f\"   RÂ² Score: {rf_r2:.4f}\")\n",
    "print(f\"   Mean Absolute Error: {rf_mae:.4f} million avocados\")\n",
    "print(f\"\\nğŸ“ˆ Improvement over single tree:\")\n",
    "print(f\"   RÂ² improved by: {(rf_r2 - tree_r2):.4f} ({(rf_r2/tree_r2 - 1)*100:+.1f}%)\")\n",
    "print(f\"   MAE reduced by: {(tree_mae - rf_mae):.4f} million ({(1 - rf_mae/tree_mae)*100:.1f}% reduction)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the improvement\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Decision Tree predictions\n",
    "axes[0].scatter(y_test, tree_pred, alpha=0.4, edgecolors='black', linewidth=0.5)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual Sales (millions)', fontsize=11)\n",
    "axes[0].set_ylabel('Predicted Sales (millions)', fontsize=11)\n",
    "axes[0].set_title(f'Decision Tree (RÂ² = {tree_r2:.3f})', fontsize=13, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Random Forest predictions\n",
    "axes[1].scatter(y_test, rf_pred, alpha=0.4, edgecolors='black', linewidth=0.5, color='green')\n",
    "axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Perfect Prediction')\n",
    "axes[1].set_xlabel('Actual Sales (millions)', fontsize=11)\n",
    "axes[1].set_ylabel('Predicted Sales (millions)', fontsize=11)\n",
    "axes[1].set_title(f'Random Forest (RÂ² = {rf_r2:.3f})', fontsize=13, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ’¡ Notice: Random Forest predictions are more tightly clustered around the red line!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” Feature Importance\n",
    "\n",
    "One of the best features of Random Forest: **interpretability through feature importance**\n",
    "\n",
    "- Measures how much each feature reduces prediction error across all trees\n",
    "- Helps answer: \"What REALLY drives avocado demand?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importance from the Random Forest model\n",
    "rf_model = rf_pipeline.named_steps['randomforestregressor']\n",
    "feature_names = (feat_transform.get_feature_names_out() \n",
    "                if hasattr(feat_transform, 'get_feature_names_out') \n",
    "                else [f'feature_{i}' for i in range(len(rf_model.feature_importances_))])\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=importance_df.head(10), x='Importance', y='Feature', palette='viridis')\n",
    "plt.title('What Drives Avocado Demand? (Top 10 Features)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Importance Score', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“Š Feature Importance Summary:\")\n",
    "print(importance_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’­ Pause for Discussion:\n",
    "\n",
    "**Q1:** Look at the feature importance. Does this make business sense?\n",
    "\n",
    "**Q2:** If you were a supplier, what ONE action would you take based on these results?\n",
    "\n",
    "**Q3:** Random Forest improved our RÂ² from ~0.85 to ~0.90. Is that improvement worth it?\n",
    "- Consider: computational cost, interpretability, model maintenance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ Section 3: Gradient Boosting - Learning from Mistakes\n",
    "\n",
    "### Different Philosophy from Random Forest\n",
    "\n",
    "**Random Forest:** Build trees independently, then average  \n",
    "**Gradient Boosting:** Build trees sequentially, where each tree learns from the previous tree's mistakes\n",
    "\n",
    "### The Core Algorithm:\n",
    "\n",
    "1. **Start simple:** Make an initial prediction (often just the mean)\n",
    "2. **Calculate residuals:** Find the errors (actual - predicted)\n",
    "3. **Build a tree to predict the residuals:** This tree focuses on correcting mistakes\n",
    "4. **Update predictions:** Add the new tree's predictions (scaled by learning rate)\n",
    "5. **Repeat:** Build the next tree on the NEW residuals\n",
    "\n",
    "### Intuitive Example:\n",
    "\n",
    "```\n",
    "Iteration 1: Predict demand = 2.5M everywhere\n",
    "            Errors: Some regions off by +0.5M, others by -0.3M\n",
    "\n",
    "Iteration 2: Build tree to predict those errors\n",
    "            New prediction = 2.5M + 0.1 * (error prediction)\n",
    "            \n",
    "Iteration 3: Calculate NEW errors, build another tree\n",
    "            New prediction = previous + 0.1 * (new error prediction)\n",
    "            \n",
    "...\n",
    "\n",
    "After 100 iterations: Very accurate predictions!\n",
    "```\n",
    "\n",
    "### Why \"Gradient\"?\n",
    "\n",
    "- Each tree is fitted to the **gradient** (derivative) of the loss function\n",
    "- This is like gradient descent, but building trees instead of updating weights\n",
    "- Mathematical optimization meets machine learning!\n",
    "\n",
    "### Key Hyperparameters:\n",
    "\n",
    "```python\n",
    "GradientBoostingRegressor(\n",
    "    n_estimators=100,      # Number of boosting stages\n",
    "    learning_rate=0.1,     # How much each tree contributes (smaller = more conservative)\n",
    "    max_depth=3,           # Shallow trees work better (typically 3-6)\n",
    "    subsample=0.8,         # Fraction of samples for each tree (adds randomness)\n",
    "    random_state=42\n",
    ")\n",
    "```\n",
    "\n",
    "### Trade-off: Learning Rate vs. Number of Trees\n",
    "\n",
    "- **Low learning rate + Many trees** = Better accuracy, but slower training\n",
    "- **High learning rate + Fewer trees** = Faster, but risk overfitting\n",
    "- Common practice: Start with lr=0.1 and n_estimators=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Gradient Boosting\n",
    "gb_pipeline = make_pipeline(\n",
    "    feat_transform,\n",
    "    GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=4, random_state=42)\n",
    ")\n",
    "gb_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "gb_pred = gb_pipeline.predict(X_test)\n",
    "gb_r2 = r2_score(y_test, gb_pred)\n",
    "gb_mae = mean_absolute_error(y_test, gb_pred)\n",
    "\n",
    "print(\"ğŸš€ Gradient Boosting Performance:\")\n",
    "print(f\"   RÂ² Score: {gb_r2:.4f}\")\n",
    "print(f\"   Mean Absolute Error: {gb_mae:.4f} million avocados\")\n",
    "print(f\"\\nğŸ“ˆ Improvement over Random Forest:\")\n",
    "print(f\"   RÂ² improved by: {(gb_r2 - rf_r2):.4f} ({(gb_r2/rf_r2 - 1)*100:+.1f}%)\")\n",
    "print(f\"   MAE reduced by: {(rf_mae - gb_mae):.4f} million ({(1 - gb_mae/rf_mae)*100:.1f}% reduction)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“Š THE BIG COMPARISON: Linear vs. Tree vs. Forest vs. Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train linear regression for comparison\n",
    "lr_pipeline = make_pipeline(feat_transform, LinearRegression())\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "lr_pred = lr_pipeline.predict(X_test)\n",
    "lr_r2 = r2_score(y_test, lr_pred)\n",
    "lr_mae = mean_absolute_error(y_test, lr_pred)\n",
    "\n",
    "# Create comparison table\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['Linear Regression', 'Decision Tree', 'Random Forest', 'Gradient Boosting'],\n",
    "    'RÂ² Score': [lr_r2, tree_r2, rf_r2, gb_r2],\n",
    "    'MAE (millions)': [lr_mae, tree_mae, rf_mae, gb_mae],\n",
    "    'Training Time': ['Fast', 'Fast', 'Medium', 'Slow'],\n",
    "    'Interpretability': ['High', 'High', 'Medium', 'Medium'],\n",
    "    'Handles Non-linearity': ['No', 'Yes', 'Yes', 'Yes']\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" \"*25 + \"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(comparison.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Visualize performance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# RÂ² comparison\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
    "bars = axes[0].bar(comparison['Model'], comparison['RÂ² Score'], color=colors, alpha=0.8, edgecolor='black')\n",
    "axes[0].set_ylabel('RÂ² Score', fontsize=12)\n",
    "axes[0].set_title('Prediction Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylim([0.7, 1.0])\n",
    "axes[0].tick_params(axis='x', rotation=15)\n",
    "for i, bar in enumerate(bars):\n",
    "    height = bar.get_height()\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{height:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# MAE comparison\n",
    "bars = axes[1].bar(comparison['Model'], comparison['MAE (millions)'], color=colors, alpha=0.8, edgecolor='black')\n",
    "axes[1].set_ylabel('Mean Absolute Error (millions)', fontsize=12)\n",
    "axes[1].set_title('Prediction Error Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].tick_params(axis='x', rotation=15)\n",
    "for i, bar in enumerate(bars):\n",
    "    height = bar.get_height()\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "                f'{height:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ Key Takeaway: Ensemble methods (RF and GBM) consistently outperform single models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# ğŸ¯ Part 2: Integration with Optimization\n",
    "\n",
    "## The Million Dollar Question:\n",
    "\n",
    "**\"Does a better prediction model lead to better business decisions?\"**\n",
    "\n",
    "Let's find out by plugging each model into our price optimization framework!\n",
    "\n",
    "### Reminder: The Optimization Problem\n",
    "\n",
    "**Decision Variables:**\n",
    "- $p_r$ = price per avocado in region $r$\n",
    "- $x_r$ = avocados supplied to region $r$\n",
    "- $s_r$ = avocados sold in region $r$\n",
    "- $d_r$ = predicted demand in region $r$\n",
    "\n",
    "**Objective:**\n",
    "$$\\max \\sum_r (p_r \\cdot s_r - c_{waste} \\cdot w_r - c^r_{transport} \\cdot x_r)$$\n",
    "\n",
    "**Key Constraints:**\n",
    "- Supply constraint: $\\sum_r x_r = B$\n",
    "- Sales relationship: $s_r \\leq x_r$ and $s_r \\leq d_r(p_r, \\text{region}, \\text{year}, \\text{peak})$\n",
    "- Wastage: $w_r = x_r - s_r$\n",
    "\n",
    "**The critical link:** $d_r$ is now predicted by our ML model! Better predictions â†’ Better optimal decisions!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ Build the Optimization Model (Using Random Forest)\n",
    "\n",
    "We'll use Gurobi Machine Learning to embed our trained Random Forest directly into the optimization model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization parameters\n",
    "B = 35  # total avocados available (millions)\n",
    "peak_or_not = 1  # 1 if peak season, 0 if not\n",
    "year = 2022\n",
    "c_waste = 0.1  # cost per wasted avocado ($)\n",
    "\n",
    "# Transportation costs by region\n",
    "c_transport = pd.Series({\n",
    "    \"Great_Lakes\": 0.3,\n",
    "    \"Midsouth\": 0.1,\n",
    "    \"Northeast\": 0.4,\n",
    "    \"Northern_New_England\": 0.5,\n",
    "    \"SouthCentral\": 0.3,\n",
    "    \"Southeast\": 0.2,\n",
    "    \"West\": 0.2,\n",
    "    \"Plains\": 0.2,\n",
    "}, name='transport_cost')\n",
    "c_transport = c_transport.loc[regions]\n",
    "\n",
    "# Price bounds\n",
    "a_min = 0.5  # minimum price\n",
    "a_max = 2.5  # maximum price\n",
    "\n",
    "# Delivery bounds from historical data\n",
    "data = pd.concat([\n",
    "    c_transport,\n",
    "    df.groupby(\"region\")[\"units_sold\"].min().rename('min_delivery'),\n",
    "    df.groupby(\"region\")[\"units_sold\"].max().rename('max_delivery')\n",
    "], axis=1)\n",
    "\n",
    "print(\"ğŸ“‹ Optimization Parameters:\")\n",
    "print(data)\n",
    "print(f\"\\nğŸ¯ Total supply to allocate: {B} million avocados\")\n",
    "print(f\"ğŸ“… Year: {year}, Peak season: {'Yes' if peak_or_not else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to solve optimization with any trained model\n",
    "def solve_optimization(model_pipeline, model_name):\n",
    "    \"\"\"\n",
    "    Solve the avocado price optimization using a given ML prediction model.\n",
    "    \n",
    "    Args:\n",
    "        model_pipeline: Trained sklearn pipeline (must include preprocessing + model)\n",
    "        model_name: String name for reporting\n",
    "    \n",
    "    Returns:\n",
    "        solution: DataFrame with optimal prices, allocations, sales, waste\n",
    "        objective_value: Optimal net revenue\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ğŸ¯ SOLVING OPTIMIZATION WITH: {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Create Gurobi model\n",
    "    m = gp.Model(f\"Avocado_Optimization_{model_name}\")\n",
    "    m.setParam('OutputFlag', 0)  # Suppress Gurobi output\n",
    "    \n",
    "    # Decision variables\n",
    "    p = gppd.add_vars(m, data, name=\"price\", lb=a_min, ub=a_max)\n",
    "    x = gppd.add_vars(m, data, name=\"x\", lb='min_delivery', ub='max_delivery')\n",
    "    s = gppd.add_vars(m, data, name=\"s\")\n",
    "    w = gppd.add_vars(m, data, name=\"w\")\n",
    "    d = gppd.add_vars(m, data, lb=-gp.GRB.INFINITY, name=\"demand\")\n",
    "    \n",
    "    m.update()\n",
    "    \n",
    "    # Constraints\n",
    "    m.addConstr(x.sum() == B, name=\"total_supply\")\n",
    "    gppd.add_constrs(m, s, gp.GRB.LESS_EQUAL, x, name=\"sales_limited_by_supply\")\n",
    "    gppd.add_constrs(m, s, gp.GRB.LESS_EQUAL, d, name=\"sales_limited_by_demand\")\n",
    "    gppd.add_constrs(m, w, gp.GRB.EQUAL, x - s, name=\"wastage_definition\")\n",
    "    \n",
    "    # Create features for ML model\n",
    "    feats = pd.DataFrame({\n",
    "        \"year\": year,\n",
    "        \"peak\": peak_or_not,\n",
    "        \"region\": regions,\n",
    "    }, index=regions)\n",
    "    \n",
    "    m_feats = pd.concat([feats, p], axis=1)[[\"region\", \"price\", \"year\", \"peak\"]]\n",
    "    \n",
    "    # Add predictor constraints (this is where the magic happens!)\n",
    "    pred_constr = add_predictor_constr(m, model_pipeline, m_feats, d)\n",
    "    \n",
    "    print(f\"\\nğŸ”— ML Model Integration:\")\n",
    "    pred_constr.print_stats()\n",
    "    \n",
    "    # Objective: maximize net revenue\n",
    "    m.setObjective(\n",
    "        (p * s).sum() - c_waste * w.sum() - (c_transport * x).sum(),\n",
    "        gp.GRB.MAXIMIZE\n",
    "    )\n",
    "    \n",
    "    # Solve (nonconvex due to p * s)\n",
    "    m.Params.NonConvex = 2\n",
    "    m.optimize()\n",
    "    \n",
    "    if m.status != gp.GRB.OPTIMAL:\n",
    "        print(f\"âŒ Optimization failed with status {m.status}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Extract solution\n",
    "    solution = pd.DataFrame(index=regions)\n",
    "    solution[\"Price\"] = p.gppd.X\n",
    "    solution[\"Allocated\"] = x.gppd.X\n",
    "    solution[\"Sold\"] = s.gppd.X\n",
    "    solution[\"Wasted\"] = w.gppd.X\n",
    "    solution[\"Pred_demand\"] = d.gppd.X\n",
    "    \n",
    "    opt_revenue = m.ObjVal\n",
    "    \n",
    "    print(f\"\\nâœ… OPTIMIZATION COMPLETE\")\n",
    "    print(f\"ğŸ’° Optimal Net Revenue: ${opt_revenue:.4f} million\")\n",
    "    print(f\"\\nğŸ“Š Solution Summary:\")\n",
    "    print(solution.round(4))\n",
    "    \n",
    "    # Check ML approximation error\n",
    "    max_error = np.max(pred_constr.get_error())\n",
    "    print(f\"\\nğŸ¯ Max ML approximation error: {max_error:.6f}\")\n",
    "    \n",
    "    return solution, opt_revenue\n",
    "\n",
    "print(\"âœ… Optimization function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ The Ultimate Test: Compare All Models in Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve with Linear Regression\n",
    "sol_lr, revenue_lr = solve_optimization(lr_pipeline, \"Linear Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve with Random Forest\n",
    "sol_rf, revenue_rf = solve_optimization(rf_pipeline, \"Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve with Gradient Boosting\n",
    "sol_gb, revenue_gb = solve_optimization(gb_pipeline, \"Gradient Boosting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“ˆ THE BIG REVEAL: Does Better Prediction = Better Decisions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare optimization results\n",
    "optimization_results = pd.DataFrame({\n",
    "    'Model': ['Linear Regression', 'Random Forest', 'Gradient Boosting'],\n",
    "    'Test RÂ²': [lr_r2, rf_r2, gb_r2],\n",
    "    'Optimal Revenue ($M)': [revenue_lr, revenue_rf, revenue_gb],\n",
    "    'Revenue Improvement': [\n",
    "        0,\n",
    "        revenue_rf - revenue_lr,\n",
    "        revenue_gb - revenue_lr\n",
    "    ],\n",
    "    'Improvement (%)': [\n",
    "        0,\n",
    "        (revenue_rf / revenue_lr - 1) * 100,\n",
    "        (revenue_gb / revenue_lr - 1) * 100\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\" \"*20 + \"DOES BETTER PREDICTION = BETTER DECISIONS?\")\n",
    "print(\"=\"*90)\n",
    "print(optimization_results.to_string(index=False))\n",
    "print(\"=\"*90)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Revenue comparison\n",
    "colors = ['#3498db', '#2ecc71', '#f39c12']\n",
    "bars = axes[0].bar(optimization_results['Model'], optimization_results['Optimal Revenue ($M)'], \n",
    "                   color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "axes[0].set_ylabel('Optimal Revenue ($M)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Optimal Revenue by Prediction Model', fontsize=14, fontweight='bold')\n",
    "axes[0].tick_params(axis='x', rotation=15)\n",
    "for i, bar in enumerate(bars):\n",
    "    height = bar.get_height()\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
    "                f'${height:.2f}M', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Scatter: Prediction RÂ² vs. Revenue\n",
    "axes[1].scatter(optimization_results['Test RÂ²'], optimization_results['Optimal Revenue ($M)'], \n",
    "                s=300, c=colors, alpha=0.7, edgecolors='black', linewidth=2)\n",
    "for i, model in enumerate(optimization_results['Model']):\n",
    "    axes[1].annotate(model, \n",
    "                    (optimization_results['Test RÂ²'].iloc[i], optimization_results['Optimal Revenue ($M)'].iloc[i]),\n",
    "                    xytext=(10, 10), textcoords='offset points', fontsize=10, fontweight='bold')\n",
    "axes[1].set_xlabel('Prediction RÂ² Score', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Optimal Revenue ($M)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Better Predictions â†’ Better Decisions', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate business impact\n",
    "best_revenue = optimization_results['Optimal Revenue ($M)'].max()\n",
    "baseline_revenue = revenue_lr\n",
    "improvement_dollars = best_revenue - baseline_revenue\n",
    "improvement_pct = (improvement_dollars / baseline_revenue) * 100\n",
    "\n",
    "print(f\"\\nğŸ’¡ KEY BUSINESS INSIGHT:\")\n",
    "print(f\"   Using the best ML model (vs. basic linear regression) generates:\")\n",
    "print(f\"   ğŸ“ˆ ${improvement_dollars:.4f} million MORE in net revenue\")\n",
    "print(f\"   ğŸ“ˆ {improvement_pct:.2f}% improvement\")\n",
    "print(f\"\\n   Over a year (52 weeks), this could mean:\")\n",
    "print(f\"   ğŸ’° ${improvement_dollars * 52:.2f} million additional revenue!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” Deep Dive: How Do Optimal Prices Differ?\n",
    "\n",
    "Let's examine how different prediction models lead to different pricing strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare optimal prices across models\n",
    "price_comparison = pd.DataFrame({\n",
    "    'Region': regions,\n",
    "    'LR_Price': sol_lr['Price'].values,\n",
    "    'RF_Price': sol_rf['Price'].values,\n",
    "    'GB_Price': sol_gb['Price'].values\n",
    "})\n",
    "\n",
    "price_comparison['RF_vs_LR'] = price_comparison['RF_Price'] - price_comparison['LR_Price']\n",
    "price_comparison['GB_vs_LR'] = price_comparison['GB_Price'] - price_comparison['LR_Price']\n",
    "\n",
    "print(\"\\nğŸ“Š OPTIMAL PRICE COMPARISON BY REGION:\")\n",
    "print(price_comparison.round(4))\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(regions))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = ax.bar(x - width, price_comparison['LR_Price'], width, label='Linear Reg', \n",
    "               color='#3498db', alpha=0.8, edgecolor='black')\n",
    "bars2 = ax.bar(x, price_comparison['RF_Price'], width, label='Random Forest',\n",
    "               color='#2ecc71', alpha=0.8, edgecolor='black')\n",
    "bars3 = ax.bar(x + width, price_comparison['GB_Price'], width, label='Gradient Boosting',\n",
    "               color='#f39c12', alpha=0.8, edgecolor='black')\n",
    "\n",
    "ax.set_xlabel('Region', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Optimal Price ($)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('How Model Choice Affects Optimal Pricing Strategy', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(regions, rotation=45, ha='right')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ’­ INTERPRETATION:\")\n",
    "print(f\"   Different ML models capture different demand patterns\")\n",
    "print(f\"   â†’ Lead to different optimal pricing strategies\")\n",
    "print(f\"   â†’ Result in different revenue outcomes\")\n",
    "print(f\"\\n   More accurate demand prediction = More profitable price optimization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# ğŸ“ Part 3: Best Practices & Critical Thinking\n",
    "\n",
    "## âš–ï¸ The Optimization Trade-off: It's Not Always About Maximum Accuracy\n",
    "\n",
    "### Key Dimensions to Consider:\n",
    "\n",
    "| Dimension | Linear Regression | Random Forest | Gradient Boosting |\n",
    "|-----------|------------------|---------------|-------------------|\n",
    "| **Prediction Accuracy** | â­â­â­ | â­â­â­â­ | â­â­â­â­â­ |\n",
    "| **Optimization Speed** | â­â­â­â­â­ | â­â­â­ | â­â­ |\n",
    "| **Model Interpretability** | â­â­â­â­â­ | â­â­â­ | â­â­â­ |\n",
    "| **Implementation Complexity** | â­â­â­â­â­ | â­â­â­ | â­â­ |\n",
    "| **Maintenance Burden** | â­â­â­â­â­ | â­â­â­ | â­â­ |\n",
    "| **Stakeholder Trust** | â­â­â­â­â­ | â­â­â­ | â­â­ |\n",
    "\n",
    "### Real-World Considerations:\n",
    "\n",
    "#### 1. **Computational Constraints**\n",
    "- Gurobi ML must represent tree-based models as linear constraints\n",
    "- 100 trees Ã— 10 splits per tree = 1,000+ additional constraints\n",
    "- For large-scale problems, this can make optimization intractable\n",
    "\n",
    "#### 2. **The \"Good Enough\" Principle**\n",
    "- If Linear Regression gives 95% of the optimal revenue with 1/10th the solve time...\n",
    "- Is the extra 5% worth the complexity?\n",
    "- **Business question, not just technical question!**\n",
    "\n",
    "#### 3. **Model Maintenance & Drift**\n",
    "- Complex models need more frequent retraining\n",
    "- Random Forest/GBM hyperparameters need tuning\n",
    "- Linear regression coefficients are easier to monitor\n",
    "\n",
    "#### 4. **Explainability to Stakeholders**\n",
    "- CFO asks: \"Why did we set the Northeast price at $1.85?\"\n",
    "- Linear Regression: \"For every $0.10 increase in price, demand drops by X units\"\n",
    "- Random Forest: \"Um... 100 trees voted and...\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Decision Framework: When to Use What?\n",
    "\n",
    "### Use **Linear Regression** when:\n",
    "- âœ… Relationships are approximately linear\n",
    "- âœ… You need to explain every decision to stakeholders\n",
    "- âœ… Optimization must run in real-time (< 1 second)\n",
    "- âœ… Model stability over time is critical\n",
    "- âœ… You have limited ML expertise on the team\n",
    "\n",
    "**Example:** Daily pricing updates for 10,000 products across 500 stores\n",
    "\n",
    "### Use **Random Forest** when:\n",
    "- âœ… Relationships are clearly non-linear\n",
    "- âœ… You have moderate computational resources\n",
    "- âœ… Feature importance insights are valuable\n",
    "- âœ… Stakeholders trust \"data-driven black boxes\"\n",
    "- âœ… The accuracy gain justifies added complexity\n",
    "\n",
    "**Example:** Weekly supply chain optimization for 50 products across 8 regions\n",
    "\n",
    "### Use **Gradient Boosting** when:\n",
    "- âœ… Maximum accuracy is paramount (competitions, high-value decisions)\n",
    "- âœ… Optimization runs infrequently (monthly strategic planning)\n",
    "- âœ… You have strong ML expertise and computational resources\n",
    "- âœ… The business impact of small improvements is massive\n",
    "- âœ… You can afford longer solve times\n",
    "\n",
    "**Example:** Annual strategic capacity planning for a $500M business unit\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§ª Practical Exercise: The Business Case\n",
    "\n",
    "**Scenario:** You're presenting to the VP of Supply Chain. They ask:\n",
    "\n",
    "> \"You're recommending we switch from our simple regression model to this Random Forest thing. \n",
    "> It will cost us $50K to implement and maintain annually. \n",
    "> Our current annual avocado revenue is $120 million. \n",
    "> Is it worth it?\"\n",
    "\n",
    "### Your Task: Build a Business Case\n",
    "\n",
    "Use the optimization results to calculate:\n",
    "1. **Expected annual revenue improvement**\n",
    "2. **ROI (Return on Investment)**\n",
    "3. **Payback period**\n",
    "4. **Key risks and mitigations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business case calculation\n",
    "weekly_improvement = revenue_rf - revenue_lr  # in millions\n",
    "annual_improvement = weekly_improvement * 52  # 52 weeks\n",
    "implementation_cost = 0.05  # $50K = $0.05M\n",
    "annual_maintenance_cost = 0.05  # $50K = $0.05M\n",
    "\n",
    "net_benefit_year1 = annual_improvement - implementation_cost - annual_maintenance_cost\n",
    "net_benefit_steady_state = annual_improvement - annual_maintenance_cost\n",
    "roi = (net_benefit_year1 / implementation_cost) * 100\n",
    "payback_months = (implementation_cost / (annual_improvement / 12))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" \"*25 + \"BUSINESS CASE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nğŸ“Š REVENUE IMPACT:\")\n",
    "print(f\"   Weekly improvement: ${weekly_improvement:.4f} million\")\n",
    "print(f\"   Annual improvement: ${annual_improvement:.4f} million\")\n",
    "\n",
    "print(f\"\\nğŸ’° COSTS:\")\n",
    "print(f\"   Implementation (one-time): ${implementation_cost:.2f} million\")\n",
    "print(f\"   Annual maintenance: ${annual_maintenance_cost:.2f} million\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ FINANCIAL METRICS:\")\n",
    "print(f\"   Net benefit (Year 1): ${net_benefit_year1:.4f} million\")\n",
    "print(f\"   Net benefit (subsequent years): ${net_benefit_steady_state:.4f} million\")\n",
    "print(f\"   ROI: {roi:.1f}%\")\n",
    "print(f\"   Payback period: {payback_months:.1f} months\")\n",
    "\n",
    "print(f\"\\nğŸ¯ RECOMMENDATION:\")\n",
    "if net_benefit_year1 > 0:\n",
    "    print(f\"   âœ… PROCEED with Random Forest implementation\")\n",
    "    print(f\"   âœ… Expect ${net_benefit_year1*1000:.0f}K additional profit in Year 1\")\n",
    "    print(f\"   âœ… Project pays for itself in {payback_months:.0f} months\")\n",
    "else:\n",
    "    print(f\"   âŒ NOT RECOMMENDED - Costs exceed benefits\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Sensitivity analysis\n",
    "print(f\"\\nğŸ”¬ SENSITIVITY ANALYSIS:\")\n",
    "print(f\"   If improvement is 50% of predicted: ${(annual_improvement * 0.5 - implementation_cost - annual_maintenance_cost):.4f}M net benefit\")\n",
    "print(f\"   If improvement is 150% of predicted: ${(annual_improvement * 1.5 - implementation_cost - annual_maintenance_cost):.4f}M net benefit\")\n",
    "print(f\"\\n   Even at 50% of predicted improvement, ROI is positive! âœ…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“ KEY TAKEAWAYS\n",
    "\n",
    "### 1. **The Prescriptive Analytics Mindset**\n",
    "- Prediction is a means to an end, not the end itself\n",
    "- The goal is better **decisions**, not just better **models**\n",
    "- Always connect ML metrics to business outcomes (RÂ² â†’ Revenue)\n",
    "\n",
    "### 2. **Tree-Based Models Are Powerful**\n",
    "- **Random Forest:** Stable, interpretable, good first choice for non-linear problems\n",
    "- **Gradient Boosting:** Maximum accuracy, but requires careful tuning\n",
    "- Both significantly outperform linear models on complex data\n",
    "\n",
    "### 3. **Integration with Optimization**\n",
    "- Gurobi ML seamlessly embeds sklearn models into optimization\n",
    "- Tree-based models add complexity (more constraints)\n",
    "- Trade-off: Prediction accuracy vs. Optimization speed\n",
    "\n",
    "### 4. **The \"Good Enough\" Principle**\n",
    "- Perfect is the enemy of good in production systems\n",
    "- Consider: accuracy, speed, interpretability, maintenance\n",
    "- **Best model â‰  Most complex model**\n",
    "\n",
    "### 5. **Business-Driven Decisions**\n",
    "- Calculate ROI before implementation\n",
    "- Understand stakeholder needs (CFO vs. Data Scientist)\n",
    "- Build trust through explainability\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’­ Critical Thinking Questions for Discussion\n",
    "\n",
    "### Question 1: Model Selection\n",
    "**Scenario:** You're optimizing pricing for an e-commerce platform with 10,000 products. Prices update every hour based on competitor pricing and inventory levels.\n",
    "\n",
    "- Which model would you choose: Linear Regression, Random Forest, or Gradient Boosting?\n",
    "- What are the key factors in your decision?\n",
    "- What might go wrong with each choice?\n",
    "\n",
    "### Question 2: Explainability vs. Accuracy\n",
    "**Scenario:** Your Gradient Boosting model achieves 95% accuracy, but the CEO asks, \"Why did we set this specific price?\" and you can't give a clear answer. Your Linear Regression achieves 88% accuracy but you can explain every coefficient.\n",
    "\n",
    "- Which do you deploy to production?\n",
    "- How do you handle the explainability gap?\n",
    "- Are there hybrid approaches?\n",
    "\n",
    "### Question 3: The False Precision Trap\n",
    "**Scenario:** Your model predicts demand will be 2,347,891 avocados. Your optimization sets the price at $1.8374.\n",
    "\n",
    "- Is this level of precision meaningful?\n",
    "- How do you communicate uncertainty to stakeholders?\n",
    "- Should you round optimal solutions?\n",
    "\n",
    "### Question 4: Model Drift\n",
    "**Scenario:** After 6 months in production, your Random Forest's accuracy has dropped from 92% to 78%. Market conditions changed (pandemic, competitor entry, etc.).\n",
    "\n",
    "- How do you detect this in production?\n",
    "- What's your retraining strategy?\n",
    "- How does model complexity affect maintenance burden?\n",
    "\n",
    "### Question 5: Ethical Considerations\n",
    "**Scenario:** Your model learns that certain regions tolerate higher prices. This leads to differential pricing that could be seen as discriminatory.\n",
    "\n",
    "- Is this optimization ethical?\n",
    "- How do you balance profit maximization with fairness?\n",
    "- What constraints might you add to the optimization?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ Advanced Topics (For Further Exploration)\n",
    "\n",
    "### 1. **Hyperparameter Tuning**\n",
    "- Use GridSearchCV or RandomizedSearchCV\n",
    "- Cross-validation for robust evaluation\n",
    "- But remember: overfit to test set is still overfitting!\n",
    "\n",
    "### 2. **XGBoost and LightGBM**\n",
    "- More efficient implementations of gradient boosting\n",
    "- Handle larger datasets, faster training\n",
    "- Industry standard for competitions and production\n",
    "\n",
    "### 3. **Stochastic Optimization**\n",
    "- What if demand is uncertain?\n",
    "- Robust optimization: optimize for worst-case\n",
    "- Chance constraints: \"95% probability of meeting demand\"\n",
    "\n",
    "### 4. **Multi-Stage Optimization**\n",
    "- Today: predict demand, then optimize once\n",
    "- Advanced: optimize pricing policy that adapts to realized demand\n",
    "- Stochastic programming, dynamic programming\n",
    "\n",
    "### 5. **Interpretable ML (SHAP, LIME)**\n",
    "- Explain individual predictions from black-box models\n",
    "- Build trust without sacrificing accuracy\n",
    "- Bridge between complex models and stakeholders\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š Additional Resources\n",
    "\n",
    "### Books:\n",
    "- **\"The Elements of Statistical Learning\"** - Hastie, Tibshirani, Friedman (Classic ML textbook)\n",
    "- **\"Hands-On Machine Learning\"** - AurÃ©lien GÃ©ron (Practical sklearn guide)\n",
    "- **\"Optimization in Operations Research\"** - Ronald Rardin\n",
    "\n",
    "### Documentation:\n",
    "- Gurobi Machine Learning: https://gurobi-machinelearning.readthedocs.io/\n",
    "- Scikit-learn Ensemble Methods: https://scikit-learn.org/stable/modules/ensemble.html\n",
    "- Gurobipy-pandas: https://github.com/Gurobi/gurobipy-pandas\n",
    "\n",
    "### Papers:\n",
    "- Breiman, L. (2001). \"Random Forests.\" *Machine Learning*\n",
    "- Friedman, J. H. (2001). \"Greedy Function Approximation: A Gradient Boosting Machine.\" *Annals of Statistics*\n",
    "\n",
    "### Kaggle Competitions (for practice):\n",
    "- Store Sales Forecasting\n",
    "- Demand Prediction Challenges\n",
    "- Any regression problem with business context\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Quick Review Quiz\n",
    "\n",
    "**Test your understanding:**\n",
    "\n",
    "1. **True or False:** Random Forest always outperforms Gradient Boosting.\n",
    "   - *Answer: False. GB often achieves higher accuracy, but RF can be better with limited tuning or parallel processing needs.*\n",
    "\n",
    "2. **What is the key difference between Random Forest and Gradient Boosting?**\n",
    "   - *Answer: RF builds trees independently in parallel; GB builds trees sequentially where each learns from previous errors.*\n",
    "\n",
    "3. **Why might you choose Linear Regression over Random Forest in prescriptive analytics?**\n",
    "   - *Answer: Speed, interpretability, stakeholder trust, easier maintenance, good enough accuracy for business needs.*\n",
    "\n",
    "4. **How does Gurobi ML embed tree-based models into optimization?**\n",
    "   - *Answer: Converts decision tree logic into linear constraints that Gurobi can solve.*\n",
    "\n",
    "5. **What is feature importance and why does it matter for business?**\n",
    "   - *Answer: Measures which features most influence predictions; helps prioritize data collection and informs business strategy.*\n",
    "\n",
    "6. **When is high prediction accuracy NOT the right goal?**\n",
    "   - *Answer: When stakeholders need explainability, when optimization speed matters, when maintenance burden exceeds benefit, when \"good enough\" suffices.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Final Thoughts: The Prescriptive Analytics Mindset\n",
    "\n",
    "```\n",
    "âŒ Wrong approach:\n",
    "   \"I built the most accurate model! RÂ² = 0.99!\"\n",
    "\n",
    "âœ… Right approach:\n",
    "   \"I improved revenue by $2M/year using a model stakeholders trust,\n",
    "    solves in under 5 seconds, and maintenance is straightforward.\n",
    "    RÂ² = 0.92.\"\n",
    "```\n",
    "\n",
    "### Remember:\n",
    "\n",
    "1. **Start simple, add complexity only when justified**\n",
    "2. **Connect every metric to business value**\n",
    "3. **Build trust through transparency**\n",
    "4. **Production-ready > Kaggle-winner**\n",
    "5. **Better decisions > Better predictions**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“¬ Thank You!\n",
    "\n",
    "**Tonight you learned:**\n",
    "- âœ… How tree-based ensemble methods work\n",
    "- âœ… When to use Random Forest vs. Gradient Boosting vs. Linear Regression\n",
    "- âœ… How to integrate ML models into optimization frameworks\n",
    "- âœ… How to evaluate models through a business lens\n",
    "- âœ… The critical thinking needed for prescriptive analytics\n",
    "\n",
    "**Next steps:**\n",
    "- Practice with your own datasets\n",
    "- Experiment with hyperparameter tuning\n",
    "- Try XGBoost and LightGBM\n",
    "- Build a complete end-to-end prescriptive analytics project\n",
    "\n",
    "### Questions? Let's discuss! ğŸ¤”\n",
    "\n",
    "---\n",
    "\n",
    "*\"The goal is to turn data into information, and information into insight, and insight into better decisions.\"*  \n",
    "â€” Carly Fiorina, Former CEO of HP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
